{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uJCioevSmGc"
   },
   "source": [
    "# [IAPR][iapr]: Lab 2 ‒  Object description\n",
    "\n",
    "**Group ID:** 37\n",
    "\n",
    "**Author 1 (sciper):** Elias De Smijter (366670)  \n",
    "**Author 2 (sciper):** Félicie Alice Agnès Marie Giraud-Sauveur (284220)   \n",
    "**Author 3 (sciper):** Cyril Felix Monette (299554)\n",
    "\n",
    "**Release date:** 24.03.2023  \n",
    "**Due date:** 07.04.2023 (11:59 pm)\n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class well as\n",
    "preparation for the final project, which is a practical project which ties together the topics of the course.\n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external\n",
    "functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation\n",
    "in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook <font color='red'> rerun </font>the notebook from scratch !**\n",
    "`Kernel` > `Restart & Run All`\n",
    "\n",
    "We will not rerun the notebook for you.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M313pak5SmGi"
   },
   "source": [
    "---\n",
    "## 0. Extract relevant data\n",
    "We first need to extract the `lab-02-data.tar.gz` archive.\n",
    "To this end, we use the [tarfile] module from the Python standard library. In the `lab-02-data` folder, you will find 28x28 grey-scale pictures of handwritten \"0\", \"1\", \"2\" and \"3\".\n",
    "These digits have been extracted from MNIST dataset (http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "[tarfile]: https://docs.python.org/3.6/library/tarfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import random\n",
    "\n",
    "import skimage.io\n",
    "from skimage.filters import median\n",
    "from skimage.morphology import closing, opening, disk, remove_small_holes, remove_small_objects, square, erosion\n",
    "from skimage.measure import inertia_tensor_eigvals\n",
    "\n",
    "from scipy.signal import wiener\n",
    "from scipy import interpolate\n",
    "import scipy\n",
    "\n",
    "import cv2\n",
    "\n",
    "import imutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeja8DMQSmGk"
   },
   "outputs": [],
   "source": [
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-02-data'\n",
    "data_part1 = os.path.join(data_base_path, data_folder, 'part1')\n",
    "data_part2 = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYsGqDm2SmGo"
   },
   "source": [
    "### 0.1.Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwZJcmXFSmGp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load(path, digit='0'):\n",
    "    digit_path = os.path.join(path, digit)\n",
    "    digit_names = [nm for nm in os.listdir(digit_path) if '.png' in nm]  # make sure to only load .png\n",
    "    digit_names.sort()  # sort file names\n",
    "    ic = skimage.io.imread_collection([os.path.join(digit_path, nm) for nm in digit_names])\n",
    "    digit_im = skimage.io.concatenate_images(ic)\n",
    "    return digit_im, digit_names\n",
    "                        \n",
    "# Load digits data\n",
    "\n",
    "# Zero images arrays\n",
    "zeros_im, zeros_names = load(data_part1, digit='0')\n",
    "# Ones images arrays\n",
    "ones_im, ones_names = load(data_part1, digit='1')\n",
    "# Twos images arrays\n",
    "twos_im, twos_names = load(data_part2, digit='2')\n",
    "# Threes images arrays\n",
    "threes_im, threes_names = load(data_part2, digit='3')\n",
    "\n",
    "# Plot images\n",
    "fig, axes = plt.subplots(4, len(zeros_im), figsize=(20, 8))\n",
    "for ax, im, nm in zip(axes[0], zeros_im, zeros_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "for ax, im, nm in zip(axes[1], ones_im, ones_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "for ax, im, nm in zip(axes[2], twos_im, twos_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "for ax, im, nm in zip(axes[3], threes_im, threes_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpWlhTRnSmGn"
   },
   "source": [
    "---\n",
    "## Part 1 - contour based descriptors (18 pts)\n",
    "\n",
    "In this part you will use images of \"0\"s and \"1\"s.\n",
    "\n",
    "**Objectives overview**: Fourier Descriptors\n",
    "\n",
    "\n",
    "**1)** *Preprocess*: Select ONLY \"0\" and \"1\" images and apply suitable preprocessing. Show a plot with the preprocessed data and give arguments on the chosen techniques. (**2 pts**)\n",
    "\n",
    "\n",
    "**2)** *Get descriptors*: Find the contours and get the Fourier descriptors in complex definition. *Note*: The contours arrays should contain a finite number (N-> self-chosen) of equi-distant points for each image (hint: interpolation). In the case of the \"0\" contours, just keep the outermost contour. Discuss the necessity of this format. (**4 pts**)\n",
    "    \n",
    "    \n",
    "**3)** *Study effect of descriptors* : Show the contour-reconstruction of the digits using different amount of descriptors (both for ONE \"0\" image and ONE \"1\" image)\n",
    "- Make 5 plots, reconstructing the digits' countours using different amounts of descriptors. (e.g. reconstruct the contour using the only the 1st fourier descriptor, reconstruct using the first two fourier descriptors,  using the first three ...)  \n",
    "- **Hint**: check scipy fft, ifft, and fftfreq functions (see [link](https://docs.scipy.org/doc/scipy/tutorial/fft.html)) and pay attention on the ordering, observing which coefficients correspond to positive/negative frequencies of components and how you choose your low frequency subsets . \n",
    "- **Extra hint** To reconstruct the contour, copy the original vector of the fourier descriptors, keep the positive an negative frequencies you will use, and set to 0 the rest. (e.g. If you want to reconstruc the image with the first _three fourier descriptors_, keep the first _three positive frecuencies_ and their respective _three negative frequencies_ (see positive and negative bin freque\n",
    "    frequencies [link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.fftfreq.html#scipy.fft.fftfreq) ). \n",
    "- Discuss about your findings. (**4 pts**)\n",
    "    \n",
    "        \n",
    "**4)** *Clustering*: For each image containing a \"0\" or a \"1\", using the fourier descriptors, extract a 2-dimensional feature vector (using the THE POSITIVE bin frequencies from fftfreq, see [link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.fftfreq.html#scipy.fft.fftfreq) ) and plot all of them on a 2D graph. Did you obtain a plot where the vectors of the \"0\"'s nicely cluster in one part of the plane and those of the \"1\"'s in the other? If yes, explain why. (**2 pts**)\n",
    "    \n",
    "    \n",
    "**5)** *Show translation, rotation and scale invariance*\n",
    "- Firstly, discuss which descriptors, or which part of the descriptors are affected by each transformation. For each transformation define (write in words) an operation which will be applied to the descriptor arrays, such that they will be invariant to (not affected by) the specific transformation anymore. After finding the necessary operations, define a function that will make the Fourier descriptors invariant to all the transformations, altogether.(**3 pts**) \n",
    "- Secondly, show that if you are using descriptors invariant to transformations, you still obtain 2 nicely defined clusters for the 2 categories of numbers, under transformations applied. (You need to define custom functions for each transformation, apply them on images, and compare the obtained invariant descriptors). Discuss your findings. (**3 pts**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUMViQWYIa1l"
   },
   "source": [
    "### 1. Fourier Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsoAfO-jIa1l"
   },
   "source": [
    "### 1.1. Preprocess and visualization (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGa7RuviIa1l"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "\n",
    "    # lists to fill with the preprocessed images of \"0\"s and \"1\"s\n",
    "zeros = []\n",
    "ones = []\n",
    "\n",
    "    # pre-process, fill lists and plot\n",
    "\n",
    "fig, axes = plt.subplots(2, len(zeros_im), figsize=(20, 5))\n",
    "\n",
    "# For zeros\n",
    "for ax, im, nm in zip(axes[0], zeros_im, zeros_names):\n",
    "    im_preprocessed = wiener(median(im.copy()),3)\n",
    "    im_preprocessed = np.where(im_preprocessed>0, 255, 0)\n",
    "    zeros.append(im_preprocessed)\n",
    "    ax.imshow(im_preprocessed, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "\n",
    "# For ones\n",
    "for ax, im, nm in zip(axes[1], ones_im, ones_names):\n",
    "    im_preprocessed = wiener(remove_small_objects(cv2.convertScaleAbs(median(im.copy())), min_size=3), 3)\n",
    "    im_preprocessed = np.where(im_preprocessed>0, 255, 0)\n",
    "    ones.append(im_preprocessed)\n",
    "    ax.imshow(im_preprocessed, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "\n",
    "fig.suptitle(\"Preprocessed images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flIcIulQSmGs",
    "tags": []
   },
   "source": [
    "#### Discussion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OSyYPFDrIa1n"
   },
   "source": [
    "- **For the zeros: We first applied a median filter with a window of 3x3. This filter allows to remove the spiky noise around the numbers and sometimes near the edges. This filter also has the advantage of preserves sharp edges, which is interesting here. We then applied a Wiener filter. This type of filter was chosen because it allows to accentuate the contours, which will be useful for the following, but taking into account the noise to avoid amplifying it too. Then we binarize the image.**\n",
    "\n",
    "- **For the ones: We used the same procedure as for the zeros but added a \"removed small objects\" step between the median and the wiener filtering. The main reason for this is image 1_8.png where there are some artifacts around the written one that this extra step removes. Then we binarize the image.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYSk6xjrSmGq"
   },
   "source": [
    "### 1.2. Get descriptors (4 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contours, fill lists and plot\n",
    "# For zeros, we just keep the outermost contour\n",
    "# We also use interpolation on the contours\n",
    "\n",
    "fig, axes = plt.subplots(4, len(zeros), figsize=(20, 8))\n",
    "\n",
    "\n",
    "# For zeros\n",
    "for ax0, ax1, im, nm in zip(axes[0], axes[1], zeros, zeros_names):\n",
    "    \n",
    "    # find contours\n",
    "    im_contours = im.copy()\n",
    "    im_contours = cv2.convertScaleAbs(im_contours)\n",
    "    Co, _ = cv2.findContours(im_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    im_contours = cv2.cvtColor(im_contours, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # interpolate contours\n",
    "    xco = Co[0].T[0][0]\n",
    "    yco = Co[0].T[1][0]\n",
    "    data = np.array(list(zip(xco, yco)))\n",
    "    tck,u = interpolate.splprep(data.transpose(), s=1, per=1) # Find splines that fit the 2D points\n",
    "    unew = np.linspace(0, 1, 100)\n",
    "    out = interpolate.splev(unew, tck) # Evaluate the splines at points of interest\n",
    "    \n",
    "    # show contours\n",
    "    ax0.imshow(cv2.drawContours(im_contours, Co, -1, (255, 0, 0), 1))\n",
    "    ax1.plot(out[0], -out[1], color='orange')  \n",
    "    ax0.axis('off')\n",
    "    ax0.set_title(nm)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(nm)\n",
    "\n",
    "    \n",
    "# For ones\n",
    "for ax0, ax1, im, nm in zip(axes[2], axes[3], ones, ones_names):\n",
    "    \n",
    "    # find contours\n",
    "    im_contours = im.copy()\n",
    "    im_contours = cv2.convertScaleAbs(im_contours)\n",
    "    Co, _ = cv2.findContours(im_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    im_contours = cv2.cvtColor(im_contours, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # interpolate contours\n",
    "    xco = Co[0].T[0][0]\n",
    "    yco = Co[0].T[1][0]\n",
    "    data = np.array(list(zip(xco, yco)))\n",
    "    tck,u = interpolate.splprep(data.transpose(), s=1, per=1)\n",
    "    unew = np.linspace(0, 1, 100)\n",
    "    out = interpolate.splev(unew, tck)\n",
    "    \n",
    "    # show contours\n",
    "    ax0.imshow(cv2.drawContours(im_contours, Co, -1, (255, 0, 0), 1))\n",
    "    ax1.plot(out[0], -out[1], color='orange')\n",
    "    ax0.axis('off')\n",
    "    ax0.set_title(nm)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(nm)\n",
    "\n",
    "fig.suptitle(\"Outer contours (first contours in red / interpolated contours in orange)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0GqguTVSmGt"
   },
   "outputs": [],
   "source": [
    "# Get the Fourier descriptors in complex definition\n",
    "\n",
    "def get_descriptors(im):\n",
    "    \"\"\"Function that takes as input the image and returns a set of descriptors.\"\"\"\n",
    "    \n",
    "    # find contours\n",
    "    im_contours = im.copy()\n",
    "    im_contours = cv2.convertScaleAbs(im_contours)\n",
    "    Co, _ = cv2.findContours(im_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # interpolate contours\n",
    "    xco = Co[0].T[0][0]\n",
    "    yco = Co[0].T[1][0]\n",
    "    data = np.array(list(zip(xco, yco)))\n",
    "    tck,u = interpolate.splprep(data.transpose(), s=1, per=1, k=3)\n",
    "    unew = np.linspace(0, 1, 100)\n",
    "    out = interpolate.splev(unew, tck)\n",
    "    \n",
    "    # Transform coordinates to complex numbers\n",
    "    fft_in = out[0] + out[1]*1.0j\n",
    "    \n",
    "    # Fourier transform\n",
    "    descriptors = scipy.fft.fft(fft_in)\n",
    "    \n",
    "    return descriptors\n",
    "\n",
    "\n",
    "# Check first 10 descriptors of a test image (just to check that your pipeline works well)\n",
    "print(get_descriptors(ones[8])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2cUQEFDIa1n"
   },
   "source": [
    "#### Discussion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mfu8aQUVIa1o"
   },
   "source": [
    "- **Why the contours array should contain a finite number of equi-distant points for each image?**\n",
    "    - The points should be equi-distant, because otherwise the 'reference-frequency' is not the same across the entire image. Example: if two pairs of points spatially differ with the same distance, the fourier transform will assign the same frequency to both pairs because this is the only information the algorithm gets. If these pairs have different distances in the real image however (this information can't be given to the function), they should not have the same frequency! The only way to solve this, is to give equi-distant points to the algorithm.  \n",
    "- **In the case of \"0\" contours, why only the outermost contour is kept?**\n",
    "    - The main difference between 0's and 1's is that a one has high-frequency components at the top & bottom of the number. A 0 is more round, which makes for lower frequencies. To make this difference as big as possible, we only use the outer contour of a 0 because it has a lower frequency than the inner contour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KsOrYBnIa1o"
   },
   "source": [
    "### 1.3. Study effect of descriptors (6 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQGFoAFcIa1o"
   },
   "outputs": [],
   "source": [
    "# Define increasing set sizes of the descriptors. You can change these numbers.\n",
    "descr_set_sizes = [1, 2, 5, 10, 20]\n",
    "\n",
    "# Define images for which you will show the recovery. You can change the id of the image.\n",
    "id_zero_im = 2\n",
    "id_one_im = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_c5Bh-JKIa1o"
   },
   "outputs": [],
   "source": [
    "# Get the Fourier descriptors in complex definition for the chosen images\n",
    "descriptors_zero = get_descriptors(zeros[id_zero_im])\n",
    "descriptors_one = get_descriptors(ones[id_one_im])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5L2INY9qIa1o"
   },
   "outputs": [],
   "source": [
    "# Study the effect of descriptors:\n",
    "# On each of the 5 plots you will show the recovery of the digits' countours using different amounts of descriptors\n",
    "# for the zero chosen image and for the one chosen image (using the same axis for both a \"0\" and a \"1\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "# Plot recovery for a zero image\n",
    "for i in range(0,  5):\n",
    "    fourier = descriptors_zero.copy()\n",
    "    # keep only positive and negative frequencies we want to use, and set the rest to 0\n",
    "    fourier[descr_set_sizes[i]+1:-descr_set_sizes[i]] = 0\n",
    "    toplot = scipy.fft.ifft(fourier)\n",
    "    xPlot = np.real(toplot)\n",
    "    yPlot = np.imag(toplot)\n",
    "    axes[i].plot(xPlot, yPlot)\n",
    "    axes[i].set_title(\"Using \"+str(descr_set_sizes[i])+\" Fourier descriptors\")\n",
    "\n",
    "# Plot recovery for a one image\n",
    "for i in range(0,  5):\n",
    "    fourier = descriptors_one.copy()\n",
    "    # keep only positive and negative frequencies we want to use, and set to 0 the rest\n",
    "    fourier[descr_set_sizes[i]+1:-descr_set_sizes[i]] = 0\n",
    "    toplot = scipy.fft.ifft(fourier)\n",
    "    xPlot = np.real(toplot)\n",
    "    yPlot = np.imag(toplot)\n",
    "    axes[i].plot(xPlot, yPlot)\n",
    "    axes[i].set_title(\"Using \"+str(descr_set_sizes[i])+\" Fourier descriptors\")\n",
    "\n",
    "fig.suptitle(\"Recovery of the contours (zero in blue / one in orange)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LJh2eu1Ia1p"
   },
   "source": [
    "#### Discussion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QtDOcXSLIa1p"
   },
   "source": [
    "**We can see that by using only the first Fourier descriptors (in addition to the 0-frequency offset), we find very well the general shape of the zero and the one. Moreover, taking only the very first Fourier descriptor(s) allows to have only the general shape, without the small \"waves\" which disturb the contours. The more descriptors we use for reconstruction, the more wavy the object becomes as we're adding more and more frequencies. The information in these first descriptor(s) is almost always enough to determine whether the symbol is a one or a zero, so we can classify based on less information.**\n",
    "\n",
    "**This is also the case when testing onto other images from the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TI2byeUMIa1p"
   },
   "source": [
    "### 1.4 Clustering (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJ28nvPoIa1p"
   },
   "outputs": [],
   "source": [
    "# Clustering: using the fourier descriptors of the zero and the one image, \n",
    "# extract a 2-dimensional feature vector (using the POSITIVE bin frequencies)\n",
    "# and plot all of them on a 2D graph\n",
    "\n",
    "# We will use the the magnitude of the first two Fourier descriptors\n",
    "\n",
    "# First feature for all \"0\"s\n",
    "zeros_f1 = [np.absolute(get_descriptors(im)[0]) for im in zeros]\n",
    "# Second feature for all \"0\"s\n",
    "zeros_f2 = [np.absolute(get_descriptors(im)[1]) for im in zeros]\n",
    "\n",
    "# Fist feature for all \"1\"s\n",
    "ones_f1 = [np.absolute(get_descriptors(im)[0]) for im in ones]\n",
    "# Second feature for all \"1\"s\n",
    "ones_f2 = [np.absolute(get_descriptors(im)[1]) for im in ones]\n",
    "\n",
    "# Plot features for all images\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.scatter(zeros_f1, zeros_f2, color='r', label='zeros')\n",
    "ax.scatter(ones_f1, ones_f2, color='b', label='ones')\n",
    "for i in range(10):\n",
    "    ax.annotate(i, (zeros_f1[i], zeros_f2[i]))\n",
    "for i in range(10):\n",
    "    ax.annotate(i, (ones_f1[i], ones_f2[i]))\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlabel('A1')\n",
    "ax.set_ylabel('A2')\n",
    "plt.title(\"Features based on the magnitude of the first two Fourier descriptors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o17jvjJgSmGu"
   },
   "source": [
    "#### Discussion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1PGKnUMeIa1q"
   },
   "source": [
    "**We obtain a plot where the vectors of the \"0\"'s nicely cluster in one part of the plane and those of the \"1\"'s in the other. This is rather logical since we have seen previously that the first two Fourier descriptors (0-frequency desciptor and the next one) allowed us to have the general shape and thus to distinguish well between a zero and a one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvosSyDyIa1q"
   },
   "source": [
    "### 1.5 Transformation invariance (6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiMViCPlIa1q"
   },
   "source": [
    "#### Preliminary discussion: Fourier descriptors and effect of a translation, a rotation or a scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbZbjWWDIa1q"
   },
   "source": [
    "- **The translation affects the first Fourier descriptor. In order to be translation invariant, the very first Fourier descriptor must be ignored.**\n",
    "- **The rotation affects the phase of the Fourier descriptors by the same amount. To be invariant to the rotation, we will take only the amplitude of each Fourier descriptor, which is not modified by the rotation.**\n",
    "- **The scaling affects the Fourier descriptors by multiplying each of them by the same value. To be invariant to the scaling, we will take the ratio between Fourier descriptors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KMrdqevIa1q"
   },
   "source": [
    "#### Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SCK9R1BIa1q"
   },
   "outputs": [],
   "source": [
    "# Function to be invariant in translation and/or rotation and/or scaling\n",
    "\n",
    "def get_invariant_features(im, inv_to_translation=False, inv_to_rotation=False, inv_to_scale=False):\n",
    "    \"\"\"Implement a function that takes as input an image \n",
    "    and outputs a set of features invariant to scale, rotation and translation,\n",
    "    according to function boolean parameters.\"\"\"\n",
    "    \n",
    "    descriptors = get_descriptors(im)\n",
    "    \n",
    "    if inv_to_translation:\n",
    "        # remove the first Fourier descriptor\n",
    "        descriptors = descriptors[1:]\n",
    "    \n",
    "    if inv_to_rotation:\n",
    "        # take only the amplitude of the Fourier descriptors\n",
    "        descriptors = np.absolute(descriptors)\n",
    "    \n",
    "    if inv_to_scale:\n",
    "        # take the ratio of Fourier descriptors\n",
    "        descriptors = descriptors[1:]/descriptors[:-1]\n",
    "        \n",
    "    return np.absolute(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM-7YB8jSmGv"
   },
   "outputs": [],
   "source": [
    "# Define functions to apply transformations to an image\n",
    "\n",
    "\n",
    "# Define translation function\n",
    "def random_translate(im):\n",
    "    \"\"\"Translate the image\"\"\"\n",
    "    transfo_im = im.copy()\n",
    "    transfo_im = np.asarray(transfo_im, dtype=np.float32)\n",
    "    height, width = transfo_im.shape\n",
    "    trnsl_height, trnsl_width = height*random.randint(-3,3)/30, width*random.randint(-3,3)/30\n",
    "    T = np.float32([[1, 0, trnsl_width], [0, 1, trnsl_height]])\n",
    "    transfo_im = cv2.warpAffine(transfo_im, T, (width, height))\n",
    "    return np.where(transfo_im!=0, 255, 0)\n",
    "\n",
    "\n",
    "# Define rotation function\n",
    "def random_rotate(im):\n",
    "    \"\"\"Rotate the image\"\"\"\n",
    "    transfo_im = im.copy()\n",
    "    transfo_im = scipy.ndimage.rotate(transfo_im, random.randint(1,359), reshape=True)\n",
    "    #transfo_im = imutils.rotate(transfo_im, random.randint(1,359))\n",
    "    return np.where(transfo_im!=0, 255, 0)\n",
    "\n",
    "\n",
    "# Define scaling function\n",
    "def random_scale(im):\n",
    "    \"\"\"Change the scale of the image\"\"\"\n",
    "    transfo_im = im.copy()\n",
    "    transfo_im = np.asarray(transfo_im, dtype=np.float32)\n",
    "    scale_percent = random.uniform(0.5,1.50)\n",
    "    width = int(transfo_im.shape[1]*scale_percent)\n",
    "    height = int(transfo_im.shape[0]*scale_percent)\n",
    "    dim = (width, height)\n",
    "    transfo_im = cv2.resize(transfo_im, dim)\n",
    "    return np.where(transfo_im!=0, 255, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGtIm8YHIa1r"
   },
   "outputs": [],
   "source": [
    "# Define 4 sets of new images with some transformations\n",
    "\n",
    "# Apply only translation for each \"0\" and \"1\" and store results\n",
    "translated_zeros = [random_translate(im) for im in zeros]\n",
    "translated_ones = [random_translate(im) for im in ones]\n",
    "\n",
    "# Apply only rotation for each \"0\" and \"1\" and store results\n",
    "rotated_zeros = [random_rotate(im) for im in zeros]\n",
    "rotated_ones = [random_rotate(im) for im in ones]\n",
    "\n",
    "\"\"\"\n",
    "In case of using imutils.rotate: images need to be casted to uint8:\n",
    "# Apply only rotation for each \"0\" and \"1\" and store results\n",
    "rotated_zeros = [random_rotate(im.astype(np.uint8)) for im in zeros]\n",
    "rotated_ones = [random_rotate(im.astype(np.uint8)) for im in ones]\n",
    "\"\"\"\n",
    "\n",
    "# Apply only scaling for each \"0\" and \"1\" and store results\n",
    "scaled_zeros = [random_scale(im) for im in zeros]\n",
    "scaled_ones = [random_scale(im) for im in ones]\n",
    "\n",
    "# Apply all 3 transformations sequentially, in your custom order, for each \"0\" and \"1\" and store results\n",
    "all_zeros = [random_scale(random_rotate(random_translate(im))) for im in zeros]\n",
    "all_ones = [random_scale(random_rotate(random_translate(im))) for im in ones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot transformed images for zeros\n",
    "\n",
    "fig, axes = plt.subplots(5, len(zeros), figsize=(22, 13))\n",
    "\n",
    "# Without transformation\n",
    "for i, (ax0, im, nm) in enumerate(zip(axes[0], zeros, zeros_names)):\n",
    "    ax0.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax0.set_ylabel('ini')\n",
    "    ax0.set_title(nm)\n",
    "\n",
    "# For translation\n",
    "for i, (ax1, im, nm) in enumerate(zip(axes[1], translated_zeros, zeros_names)):\n",
    "    ax1.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax1.set_ylabel('TRANSLATION')\n",
    "    ax1.set_title(nm)\n",
    "\n",
    "# For rotation\n",
    "for i, (ax2, im, nm) in enumerate(zip(axes[2], rotated_zeros, zeros_names)):\n",
    "    ax2.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax2.set_ylabel('ROTATION')\n",
    "    ax2.set_title(nm)\n",
    "\n",
    "# For scaling\n",
    "for i, (ax3, im, nm) in enumerate(zip(axes[3], scaled_zeros, zeros_names)):\n",
    "    ax3.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax3.set_ylabel('SCALING')\n",
    "    ax3.set_title(nm)\n",
    "\n",
    "# For all\n",
    "for i, (ax4, im, nm) in enumerate(zip(axes[4], all_zeros, zeros_names)):\n",
    "    ax4.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax4.set_ylabel('ALL')\n",
    "    ax4.set_title(nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot transformed images for ones\n",
    "\n",
    "fig, axes = plt.subplots(5, len(ones), figsize=(22, 13))\n",
    "\n",
    "# Without transformation\n",
    "for i, (ax0, im, nm) in enumerate(zip(axes[0], ones, ones_names)):\n",
    "    ax0.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax0.set_ylabel('ini')\n",
    "    ax0.set_title(nm)\n",
    "\n",
    "# For translation\n",
    "for i, (ax1, im, nm) in enumerate(zip(axes[1], translated_ones, ones_names)):\n",
    "    ax1.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax1.set_ylabel('TRANSLATION')\n",
    "    ax1.set_title(nm)\n",
    "\n",
    "# For rotation\n",
    "for i, (ax2, im, nm) in enumerate(zip(axes[2], rotated_ones, ones_names)):\n",
    "    ax2.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax2.set_ylabel('ROTATION')\n",
    "    ax2.set_title(nm)\n",
    "\n",
    "# For scaling\n",
    "for i, (ax3, im, nm) in enumerate(zip(axes[3], scaled_ones, ones_names)):\n",
    "    ax3.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax3.set_ylabel('SCALING')\n",
    "    ax3.set_title(nm)\n",
    "\n",
    "# For all\n",
    "for i, (ax4, im, nm) in enumerate(zip(axes[4], all_ones, ones_names)):\n",
    "    ax4.imshow(im, cmap='gray') \n",
    "    if i==0:\n",
    "        ax4.set_ylabel('ALL')\n",
    "    ax4.set_title(nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5gBHBZNIa1r"
   },
   "outputs": [],
   "source": [
    "# For each of the 4 sets of \"0\"s and \"1\"s transformed, \n",
    "# find the invariant features and make a plot (similar to section 1.4)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "transfo = [\"Translation\", \"Rotation\", \"Scaling\", \"All\"]\n",
    "inv_params = [(True, False, False), (False, True, False), (False, False, True), (True, True, True)]\n",
    "transformed_zeros = [translated_zeros, rotated_zeros, scaled_zeros, all_zeros]\n",
    "transformed_ones = [translated_ones, rotated_ones, scaled_ones, all_ones]\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    # Get invariant images\n",
    "    inv_zeros = [get_invariant_features(im, inv_params[i][0], inv_params[i][1], inv_params[i][2]) for im in transformed_zeros[i]]\n",
    "    inv_ones = [get_invariant_features(im, inv_params[i][0], inv_params[i][1], inv_params[i][2]) for im in transformed_ones[i]]\n",
    "    \n",
    "    # First feature for all \"0\"s\n",
    "    zeros_f1 = [f[0] for f in inv_zeros]\n",
    "    # Second feature for all \"0\"s\n",
    "    zeros_f2 = [f[1] for f in inv_zeros]\n",
    "    \n",
    "    # First feature for all \"1\"s\n",
    "    ones_f1 = [f[0] for f in inv_ones]\n",
    "    # Second feature for all \"1\"s\n",
    "    ones_f2 = [f[1] for f in inv_ones]\n",
    "\n",
    "    # Plot features for all images\n",
    "    axes[i].scatter(zeros_f1, zeros_f2, color='r', label='zeros', s=40)\n",
    "    axes[i].scatter(ones_f1, ones_f2, color='b', label='ones', s=40)\n",
    "    axes[i].legend(loc='best')\n",
    "    axes[i].set_xlabel('A1')\n",
    "    axes[i].set_ylabel('A2')\n",
    "    for im_a in range(10):\n",
    "        axes[i].annotate(im_a, (zeros_f1[im_a], zeros_f2[im_a]), fontsize=10)\n",
    "    for im_a in range(10):\n",
    "        axes[i].annotate(im_a, (ones_f1[im_a], ones_f2[im_a]), fontsize=10)\n",
    "    axes[i].set_title(transfo[i])\n",
    "\n",
    "fig.suptitle(\"Features based on 2 Fourier descriptors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwXYAya3SmGx"
   },
   "source": [
    "#### Discussion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "w8AsywMWIa1r"
   },
   "source": [
    "**We can see that we have a good separation between the two categories for the three transformations. However, when we combine all the transformations together, the separation between the two categories becomes harder to define. This is due to the fact that the shape is modified too far from its original shape before taking the fourier descriptors, as can be seen on the pictures above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fsqzRcUIa1r"
   },
   "source": [
    "## PART 2 - region based descriptors (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqbxtS5rIa1s"
   },
   "source": [
    "In this part, you will use the \"2\"s and \"3\"s images.\n",
    "\n",
    "**Objectives overview**:\n",
    "\n",
    "**1)** *Cluster on compacity*: Based on custom definitions of perimeter and area for each image of \"2\" and \"3\" (preprocessed - by you), make a 2D plot with 2-dimensional feature vectors, similar to section 1.4. After that, define the compacity feature for each \"2\"s and \"3\"s image. Are you still able to obtain a 1D plot with values clustered in 2 well-defined regions? Discuss your findings. (**6 pts**).\n",
    "\n",
    "**2)** *Additional method*: Choose one more region-based method (from the course or your own sources - internet allowed) and redo the 1D plot in section 2.1 (or 2D plot, depending on your choosing). Explain your method and your findings. (**4 pts**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6XdedutIa1s"
   },
   "source": [
    "### 2.3. Cluster on compacity (6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZZN3KlLIa1s"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "\n",
    "# Plot original images\n",
    "fig, axes = plt.subplots(4, len(twos_im), figsize=(20, 8))\n",
    "for ax, im, nm in zip(axes[0], twos_im, twos_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "for ax, im, nm in zip(axes[1], threes_im, threes_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "\n",
    "# Binarise images\n",
    "twos_im_b = np.zeros_like(twos_im)\n",
    "for i,im in enumerate(twos_im):\n",
    "    twos_im_b[i] = np.where(im>=90, 1, 0)\n",
    "\n",
    "threes_im_b = np.zeros_like(threes_im)\n",
    "for i,im in enumerate(threes_im):\n",
    "    threes_im_b[i] = np.where(im>=90, 1, 0)\n",
    "\n",
    "# median filtering\n",
    "twos_im_filtered = np.zeros_like(twos_im_b)\n",
    "for i,im in enumerate(twos_im_b):\n",
    "    twos_im_filtered[i] = median(im, disk(1))\n",
    "\n",
    "threes_im_filtered = np.zeros_like(threes_im_b)\n",
    "for i,im in enumerate(threes_im_b):\n",
    "    threes_im_filtered[i] = median(im, disk(1))\n",
    "\n",
    "# Plot filtered images\n",
    "for ax, im, nm in zip(axes[2], twos_im_filtered, twos_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm+\" filtered\")\n",
    "for ax, im, nm in zip(axes[3], threes_im_filtered, threes_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The preprocessing of the images consists of binarizing and applying a median filter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contour extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try a custom contour extraction\n",
    "\n",
    "sample_images = [twos_im_filtered[1],twos_im_filtered[7]]\n",
    "\n",
    "fig,axes = plt.subplots(2, 4, figsize=(10,5.5))\n",
    "\n",
    "for i,im in enumerate(sample_images):\n",
    "    \n",
    "    axes[i,0].imshow(im, cmap='gray')\n",
    "    axes[i,0].set_title(\"Original image\")\n",
    "\n",
    "    axes[i,1].imshow(closing(im,selem=disk(2)), cmap='gray')\n",
    "    axes[i,1].set_title(\"Closed image\")\n",
    "\n",
    "    axes[i,2].imshow(erosion(closing(im,selem=disk(2))), cmap='gray')\n",
    "    axes[i,2].set_title(\"Erosed image\")\n",
    "\n",
    "    axes[i,3].imshow(closing(im,selem=disk(2))-erosion(closing(im,selem=disk(2))), cmap='gray')\n",
    "    axes[i,3].set_title(\"Contour\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This way of finding contours has the disadvantage of requiring a closing before erosion to avoid having inner contours included in the final contour. However, this also impacts the outer contour by smoothing sharp angles and thus overall reducing the length of the perimeter.**\n",
    "\n",
    "**Now let's try with another contour extraction process:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With another contour extraction process\n",
    "\n",
    "image = np.copy(twos_im_filtered[7])\n",
    "contours, hierarchy = cv2.findContours(image, cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
    "border=cv2.drawContours(np.zeros_like(image), contours, 0, 1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(6,3))\n",
    "plt.axis('off')\n",
    "plt.imshow(border, cmap='gray')  # without the code, only an array displayed in the console"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a second method to extract contours, we use the built-in OpenCV function findContours.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perimeter and area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perimeter and area functions\n",
    "\n",
    "# Perimeter function using custom contour detection\n",
    "def perimeter1(image):\n",
    "    border = closing(image,selem=disk(2))\n",
    "    border = erosion(border)\n",
    "    border = closing(image,selem=disk(2))-border\n",
    "    return np.sum(border)\n",
    "\n",
    "# Perimeter function using cv2 contour detection\n",
    "def perimeter2(image):\n",
    "    contours, _ = cv2.findContours(image, cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
    "    border = cv2.drawContours(np.zeros_like(image), contours, 0, 1, 1)\n",
    "    return np.sum(border)\n",
    "\n",
    "# Area\n",
    "def area(image):\n",
    "    return np.sum(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **To determine the perimeter of a symbol, we start by calculating the border as shown previously. Because our images are binarized, the border elements have value 1 and the rest of the image has value 0. As the border-width is 1 pixel everywhere, we can just take the sum of all the pixels in the border-image to determine the perimeter.**\n",
    "- **To determine the area of a symbol, we again use the fact that the images are binarized. As a result of this we can just add all the pixels in the image together to get the area.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the arrays below with the perimeter and area as features for \"2\"s and \"3\"s images\n",
    "\n",
    "twos_f1 = [perimeter1(two_im) for two_im in twos_im_filtered]\n",
    "twos_f1_bis = [perimeter2(two_im) for two_im in twos_im_filtered]\n",
    "twos_f2 = [area(two_im) for two_im in twos_im_filtered]\n",
    "\n",
    "threes_f1 = [perimeter1(three_im) for three_im in threes_im_filtered]\n",
    "threes_f1_bis = [perimeter2(three_im) for three_im in threes_im_filtered]\n",
    "threes_f2 = [area(three_im) for three_im in threes_im_filtered]\n",
    "\n",
    "\n",
    "# Plot features for all images\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "axes[0].scatter(twos_f1, twos_f2, color='r', label='twos')\n",
    "axes[0].scatter(threes_f1, threes_f2, color='b', label='threes')\n",
    "for im_a in range(10):\n",
    "    axes[0].annotate(im_a, (twos_f1[im_a], twos_f2[im_a]), fontsize=10)\n",
    "for im_a in range(10):\n",
    "    axes[0].annotate(im_a, (threes_f1[im_a], threes_f2[im_a]), fontsize=10)\n",
    "axes[0].set_xlabel(\"Perimeter 1\")\n",
    "axes[0].set_ylabel(\"Area\")\n",
    "axes[0].legend(loc='best')\n",
    "\n",
    "axes[1].scatter(twos_f1_bis, twos_f2, color='r', label='twos')\n",
    "axes[1].scatter(threes_f1_bis, threes_f2, color='b', label='threes')\n",
    "for im_a in range(10):\n",
    "    axes[1].annotate(im_a, (twos_f1_bis[im_a], twos_f2[im_a]), fontsize=10)\n",
    "for im_a in range(10):\n",
    "    axes[1].annotate(im_a, (threes_f1_bis[im_a], threes_f2[im_a]), fontsize=10)\n",
    "axes[1].set_xlabel(\"Perimeter 2\")\n",
    "axes[1].set_ylabel(\"Area\")\n",
    "axes[1].legend(loc='best')\n",
    "\n",
    "fig.suptitle(\"Features based on perimeter and area\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From these scatter-plots it is already clear that area and perimeter are not fit for classifying twos and threes. We show this as well in the next cell where we compute the compacity of each symbol.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compacity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDf7Zq-8Ia1s"
   },
   "outputs": [],
   "source": [
    "# Make the 1D plot of the compacity\n",
    "\n",
    "def compacity(area, perimeter):\n",
    "    return perimeter*perimeter/area\n",
    "\n",
    "twos_f1 = [perimeter2(two_im) for two_im in twos_im_filtered]\n",
    "twos_f2 = [area(two_im) for two_im in twos_im_filtered]\n",
    "threes_f1 = [perimeter2(three_im) for three_im in threes_im_filtered]\n",
    "threes_f2 = [area(three_im) for three_im in threes_im_filtered]\n",
    "\n",
    "twos_c = [compacity(area, perimeter) for perimeter, area in zip(twos_f1,twos_f2)]\n",
    "threes_c = [compacity(area, perimeter) for perimeter, area in zip(threes_f1,threes_f2)]\n",
    "\n",
    "# Plot features for all images\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.hist(twos_c, label='twos', bins=10)\n",
    "plt.hist(threes_c, label='threes', bins=10)\n",
    "plt.xlabel(\"Compactness\")\n",
    "plt.ylabel(\"Number of occurences\")\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Compactness of 2s vs 3s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1qTyHWGIa1s"
   },
   "source": [
    "####  Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYeuWMPTIa1s"
   },
   "source": [
    "**This histogram is our version of the 1D-plot: the actual points are actually on the x-axis, but we use a histogram to show how many points are at each position.**\n",
    "**We can see that the compacity alone does not allow us to separate well the two categories.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOvoAQeLSmGy"
   },
   "source": [
    "### 2.2. Additional method (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have seen that the compacity does not allow to separate the two categories, so we will try another method based on elongation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNYSj4_hIa1t"
   },
   "outputs": [],
   "source": [
    "# Compute eigenvalues of images\n",
    "twos_eigs = [inertia_tensor_eigvals(two_im) for two_im in twos_im_filtered]\n",
    "threes_eigs = [inertia_tensor_eigvals(two_im) for two_im in threes_im_filtered]\n",
    "\n",
    "# Compute elongation of images\n",
    "twos_elongation = [sqrt(eig[0]/eig[1]) for eig in twos_eigs]\n",
    "threes_elongation = [sqrt(eig[0]/eig[1]) for eig in threes_eigs]\n",
    "\n",
    "# Plot elongation for all images\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.hist(twos_elongation, label='twos')\n",
    "plt.hist(threes_elongation, label='threes')\n",
    "plt.xlabel(\"Elongation\")\n",
    "plt.ylabel(\"Number of occurences\")\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Elongation of 2s vs 3s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JtSEy0AIa1t"
   },
   "source": [
    "#### Discussion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "L4BXjF6YIa1t"
   },
   "source": [
    "**We see that the separation between the two categories using elongation works much better than using compactness.**\n",
    "\n",
    "**Elongation is the square root of the ratio of the two second moments of the image around its principal axes. It gives thus a measure for how 'thin' the symbol is. In the results we can see that in general, a three has more elongation than a two.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "iapr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "68f98188a37ad78ba57e1ef4198116d5583d5a299d1aea9a6932e63f067608be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
